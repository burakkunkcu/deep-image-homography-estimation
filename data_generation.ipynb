{"cells":[{"cell_type":"markdown","metadata":{"id":"hgAj4Pn1Bgzn"},"source":["## Mount GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17131,"status":"ok","timestamp":1657475603245,"user":{"displayName":"Burak Künkçü","userId":"16031619716122977911"},"user_tz":-180},"id":"nNP1F8u7ACWs","outputId":"be688764-3030-4fb9-f043-45382128546c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"xDTCjPsybzg5"},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxsvYx0sbzva"},"outputs":[],"source":["# Dataset\n","dataset_folder_path = '/content/gdrive/MyDrive/deephomography/dataset/'\n","rho = 32\n","patch_size = 128\n","image_size = (320,240)\n","\n","# Create required folder(s)\n","!mkdir -p {dataset_folder_path}"]},{"cell_type":"markdown","metadata":{"id":"v1lmdhZm-t56"},"source":["## Download and extract COCO 2017 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609059,"status":"ok","timestamp":1657476212298,"user":{"displayName":"Burak Künkçü","userId":"16031619716122977911"},"user_tz":-180},"id":"wDulwbwh-tPg","outputId":"f6ea71e2-6223-4ec3-f760-3bef1d700592"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-07-10 17:53:21--  http://images.cocodataset.org/zips/train2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.136.113\n","Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.136.113|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19336861798 (18G) [application/zip]\n","Saving to: ‘train2017.zip’\n","\n","train2017.zip       100%[===================>]  18.01G  83.8MB/s    in 3m 48s  \n","\n","2022-07-10 17:57:10 (80.7 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n","\n","--2022-07-10 17:57:10--  http://images.cocodataset.org/zips/val2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.204.185\n","Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.204.185|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 815585330 (778M) [application/zip]\n","Saving to: ‘val2017.zip’\n","\n","val2017.zip         100%[===================>] 777.80M  82.7MB/s    in 9.9s    \n","\n","2022-07-10 17:57:20 (78.6 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n","\n","--2022-07-10 17:57:20--  http://images.cocodataset.org/zips/test2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.175.73\n","Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.175.73|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6646970404 (6.2G) [application/zip]\n","Saving to: ‘test2017.zip’\n","\n","test2017.zip        100%[===================>]   6.19G  79.6MB/s    in 77s     \n","\n","2022-07-10 17:58:38 (82.1 MB/s) - ‘test2017.zip’ saved [6646970404/6646970404]\n","\n","Extracting train2017.zip...\n","Extracting val2017.zip...\n","Extracting test2017.zip...\n","Deleting downloaded .zip files...\n"]}],"source":["!wget http://images.cocodataset.org/zips/train2017.zip\n","!wget http://images.cocodataset.org/zips/val2017.zip\n","!wget http://images.cocodataset.org/zips/test2017.zip\n","\n","!echo \"Extracting train2017.zip...\"\n","!unzip -q train2017.zip\n","\n","!echo \"Extracting val2017.zip...\"\n","!unzip -q val2017.zip\n","\n","!echo \"Extracting test2017.zip...\"\n","!unzip -q test2017.zip\n","\n","!echo \"Deleting downloaded .zip files...\"\n","!rm -f train2017.zip val2017.zip test2017.zip"]},{"cell_type":"markdown","metadata":{"id":"NiGFv9lrCuMc"},"source":["## Data generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suyjYDLuCumi"},"outputs":[],"source":["import os\n","import cv2\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Do not change these limits! They prevent bordering artifacts\n","tl_point_limits = [(rho,rho), (image_size[0]-patch_size-rho,image_size[1]-patch_size-rho)]\n","\n","# Function to generate a single sample\n","def generate_sample(image_path):\n","    # Read image file (as grayscale)\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Resize image to image_size\n","    image = cv2.resize(image, image_size, interpolation=cv2.INTER_CUBIC)\n","\n","    # Select a random patch\n","    tl_point = np.random.randint(tl_point_limits[0], tl_point_limits[1], size=2)\n","    tr_point = tl_point + (patch_size,0)\n","    br_point = tl_point + (patch_size,patch_size)\n","    bl_point = tl_point + (0,patch_size)\n","    patch_points = np.array([tl_point, tr_point, br_point, bl_point]);\n","\n","    # Extract patch\n","    patch = image[tl_point[1]:br_point[1], tl_point[0]:br_point[0]]\n","\n","    # Create a random perturbation (or H_fp)\n","    perturbation = np.random.randint(-rho, rho+1, size=(4,2))\n","    perturbed_patch_points = patch_points + perturbation\n","\n","    # Find corresponding homography and its inverse\n","    H = cv2.getPerspectiveTransform(patch_points.astype(np.float32), perturbed_patch_points.astype(np.float32))\n","    H_inv = np.linalg.inv(H)\n","\n","    # Apply inverse homography to image\n","    warped_image = cv2.warpPerspective(image, H_inv, image_size)\n","\n","    # Extract perturbed patch\n","    perturbed_patch = warped_image[tl_point[1]:br_point[1], tl_point[0]:br_point[0]]\n","\n","    # Return generated sample\n","    return patch, perturbed_patch, patch_points, perturbation\n","\n","def generate_samples_from_folder(folder_path, num_samples=0):\n","    # Create target folder\n","    target_folder_path = folder_path + 'homography'\n","    os.makedirs(target_folder_path, exist_ok=True)\n","\n","    print('Generating ' + target_folder_path + '...')\n","\n","    # List folder contents\n","    file_list = os.listdir(folder_path)\n","    if num_samples > len(file_list):\n","        file_list = random.choices(file_list, k=num_samples)\n","    elif num_samples > 0:\n","        file_list = random.sample(file_list, k=num_samples)\n","\n","    # Process files \n","    for count,file in enumerate(tqdm(file_list)):\n","        file_path = os.path.join(folder_path, file)\n","        target_file_path = os.path.join(target_folder_path, f'{count:08d}')\n","\n","        # Save generated sample\n","        np.save(target_file_path, np.array((generate_sample(file_path)), dtype=object))"]},{"cell_type":"markdown","metadata":{"id":"RN1edHgCC3S4"},"source":["## Process folders and generate new data for homography estimation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"uiyE9ji9XjuP","outputId":"931ed527-97f4-46ce-f5a0-2a433cdfbafe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating train2017homography...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118287/118287 [24:36<00:00, 80.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating val2017homography...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5000/5000 [01:02<00:00, 80.25it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Generating test2017homography...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 40670/40670 [07:55<00:00, 85.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Deleting dataset folders...\n"]}],"source":["generate_samples_from_folder(folder_path='train2017')\n","generate_samples_from_folder(folder_path='val2017')\n","generate_samples_from_folder(folder_path='test2017')\n","\n","!echo \"Deleting dataset folders...\"\n","!rm -dr train2017 val2017 test2017"]},{"cell_type":"markdown","metadata":{"id":"eT9WVCPtUpIh"},"source":["## Tarball generated datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OzsDrMzCbqt3","outputId":"2d3e5956-674f-4fd6-e3cb-526cab43a15a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tarballing train2017homography...\n","Tarballing val2017homography...\n","Tarballing test2017homography...\n","Deleting generated dataset folders...\n"]}],"source":["!echo \"Tarballing train2017homography...\"\n","!tar -cf train2017homography.tar train2017homography\n","\n","!echo \"Tarballing val2017homography...\"\n","!tar -cf val2017homography.tar val2017homography\n","\n","!echo \"Tarballing test2017homography...\"\n","!tar -cf test2017homography.tar test2017homography\n","\n","!echo \"Deleting generated dataset folders...\"\n","!rm -dr train2017homography val2017homography test2017homography"]},{"cell_type":"markdown","metadata":{"id":"sH7HCF05IHfI"},"source":["## Move tarballs to GoogleDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SBeTj-QaDoSJ","outputId":"5e75344e-08bc-431f-83f5-9b2d9b6d4fb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Moving train2017homography.tar...\n","Moving val2017homography.tar...\n","Moving test2017homography.tar...\n"]}],"source":["!echo \"Moving train2017homography.tar...\"\n","!mv train2017homography.tar {dataset_folder_path}\n","\n","!echo \"Moving val2017homography.tar...\"\n","!mv val2017homography.tar {dataset_folder_path}\n","\n","!echo \"Moving test2017homography.tar...\"\n","!mv test2017homography.tar {dataset_folder_path}"]},{"cell_type":"markdown","metadata":{"id":"9sELWIvUIDQ_"},"source":["## Unmount GoogleDrive"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qkTVaZy_H9RV","executionInfo":{"status":"ok","timestamp":1657478602200,"user_tz":-180,"elapsed":69435,"user":{"displayName":"Burak Künkçü","userId":"16031619716122977911"}}},"outputs":[],"source":["drive.flush_and_unmount()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"data_generation.ipynb","provenance":[],"authorship_tag":"ABX9TyOylVM7thEEB2rwpJUZkmsC"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}